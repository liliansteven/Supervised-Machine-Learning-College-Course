{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA7c4hBklfAG"
      },
      "source": [
        "## importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A91vfaZylfAH"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "from  sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nru16_HklfAI"
      },
      "source": [
        "## Load MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZxobk-zlfAI",
        "outputId": "790995bc-c7f7-4835-e330-84336582b16c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train: (60000, 28, 28)\n",
            "Y_train: (60000,)\n",
            "X_test:  (10000, 28, 28)\n",
            "Y_test:  (10000,)\n"
          ]
        }
      ],
      "source": [
        "#loading\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        " \n",
        "#shape of dataset\n",
        "print('X_train: ' + str(X_train.shape))\n",
        "print('Y_train: ' + str(Y_train.shape))\n",
        "print('X_test:  '  + str(X_test.shape))\n",
        "print('Y_test:  '  + str(Y_test.shape))\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "BPGKjk13lfAJ",
        "outputId": "e5f74174-424e-4425-9b2a-85747d2b7908"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#plotting\n",
        "for i in range(1):  \n",
        "    pyplot.subplot(330 + 1 + i)\n",
        "    pyplot.imshow(X_train[i], cmap=pyplot.get_cmap('gray'))\n",
        "    pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRVgS4hMlfAK"
      },
      "source": [
        "## Standardize your dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZj2FgrolfAK",
        "outputId": "6cc9ff0c-7f11-447a-a983-d4f6065cc481"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(70000, 784)\n"
          ]
        }
      ],
      "source": [
        "mean_train = np.mean(X_train, axis=0)\n",
        "std_train = np.std(X_train, axis=0)\n",
        "std_train[std_train == 0] = 1e-9\n",
        "X_train = (X_train - mean_train) / std_train\n",
        "\n",
        "mean_test = np.mean(X_test, axis=0)\n",
        "std_test = np.std(X_test, axis=0)\n",
        "std_test[std_test == 0] = 1e-9\n",
        "X_test = (X_test - mean_test) / std_test\n",
        "\n",
        "X_data = np.concatenate((X_train, X_test), axis=0)\n",
        "y_data = np.concatenate((Y_train, Y_test), axis=0)\n",
        "\n",
        "X_data = X_data.reshape(X_data.shape[0],-1)\n",
        "print(X_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm87HFatlfAM"
      },
      "source": [
        "## onehotercoder From scratch :-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0q346leT3yjC"
      },
      "outputs": [],
      "source": [
        "class OneHotEncoder():\n",
        "  def __init__(self):\n",
        "    self.categories = None\n",
        "\n",
        "  #  fit function is used to obtain the unique categories or classes present in the input data.\n",
        "  # On the basis of this, I will be able to encode the data\n",
        "  def fit(self , data):\n",
        "    self.categories = sorted(list(set(data)))\n",
        "\n",
        "  \n",
        "  def transform(self , data):\n",
        "    # in this condition, I'm just ensuring the there's no one can transform without a unique and sorted initialized categories\n",
        "     if self.categories is None:\n",
        "            raise ValueError(\"OneHotEncoder has not been fitted.\")\n",
        "\n",
        "     # Here I convert all the values in the data on the basis of the categories that were defined before, and one is placed under the category that classifies the current value, and zero otherwise\n",
        "     encoded_data = []\n",
        "     for value in data :\n",
        "      one_hot_vector  = [1 if value == category else 0 for category in self.categories]\n",
        "      encoded_data.append(one_hot_vector)\n",
        "\n",
        "     return np.array(encoded_data)\n",
        "\n",
        "  def fit_transform(self , data):\n",
        "    self.fit(data)\n",
        "    return self.transform(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "JilU0cp0pMGM",
        "outputId": "a436f178-39ed-4871-a862-48333968cf26"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ee743f1f-edee-4593-b3f4-9c664c8f4b01\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69995</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69996</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69997</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69998</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69999</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70000 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee743f1f-edee-4593-b3f4-9c664c8f4b01')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ee743f1f-edee-4593-b3f4-9c664c8f4b01 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ee743f1f-edee-4593-b3f4-9c664c8f4b01');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       0  1  2  3  4  5  6  7  8  9\n",
              "0      0  0  0  0  0  1  0  0  0  0\n",
              "1      1  0  0  0  0  0  0  0  0  0\n",
              "2      0  0  0  0  1  0  0  0  0  0\n",
              "3      0  1  0  0  0  0  0  0  0  0\n",
              "4      0  0  0  0  0  0  0  0  0  1\n",
              "...   .. .. .. .. .. .. .. .. .. ..\n",
              "69995  0  0  1  0  0  0  0  0  0  0\n",
              "69996  0  0  0  1  0  0  0  0  0  0\n",
              "69997  0  0  0  0  1  0  0  0  0  0\n",
              "69998  0  0  0  0  0  1  0  0  0  0\n",
              "69999  0  0  0  0  0  0  1  0  0  0\n",
              "\n",
              "[70000 rows x 10 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder = OneHotEncoder()\n",
        "y_data = y_data.reshape(-1,1)\n",
        "y_data = y_data.flatten()\n",
        "y_encoded = pd.DataFrame(encoder.fit_transform(y_data))\n",
        "y_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UluEhzmFofqQ"
      },
      "source": [
        "## Just Spliting the Data After Reshaping it :-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G8b3MQ09ofBx"
      },
      "outputs": [],
      "source": [
        "X_data = pd.DataFrame(X_data)\n",
        "x_train , x_test ,y_train , y_test = train_test_split(X_data, y_encoded ,test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPDrp52h0A9c"
      },
      "source": [
        " ## Implement a dynamic Neural Network from scratch. :-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mU2gFd4RuDbN"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, x, y, num_of_layers, size_of_layers):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        num_features = x.shape[0]\n",
        "\n",
        "        self.num_of_layers = num_of_layers+1\n",
        "\n",
        "        size_of_layers.insert(0,num_features)\n",
        "        self.size_of_layers = size_of_layers\n",
        "        \n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.outputs = []\n",
        "\n",
        "        print(size_of_layers)\n",
        "        # Initialize weights and biases for each layer\n",
        "        for i in range(1, self.num_of_layers):\n",
        "            weight_matrix = np.random.randn(self.size_of_layers[i], self.size_of_layers[i-1])\n",
        "            bias_vector = np.random.randn(self.size_of_layers[i], 1)\n",
        "            self.weights.append(weight_matrix)\n",
        "            self.biases.append(bias_vector)\n",
        "\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        sigmoid_x = self.sigmoid(x)\n",
        "        return sigmoid_x * (1 - sigmoid_x)\n",
        "\n",
        "    def mse_loss(self, y_true, y_pred):\n",
        "        return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "    def forward_pass(self, x):\n",
        "        self.outputs = []\n",
        "        output = x\n",
        "\n",
        "        # Forward pass through each layer\n",
        "        for i in range(self.num_of_layers - 1):\n",
        "            input_data = output\n",
        "            weight_matrix = self.weights[i]\n",
        "            bias_vector = self.biases[i]\n",
        "            z = np.dot(weight_matrix, input_data) + bias_vector\n",
        "            output = self.sigmoid(z)\n",
        "            self.outputs.append((input_data, z, output))\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward_pass(self, x, y):\n",
        "        num_samples = x.shape[1]\n",
        "        deltas = []\n",
        "        derivatives_w = []\n",
        "        derivatives_b = []\n",
        "\n",
        "        # Calculate the delta of the output layer\n",
        "        input_data, z, output = self.outputs[-1]\n",
        "        delta = (output - y) * self.sigmoid_derivative(z)\n",
        "        deltas.append(delta)\n",
        "\n",
        "        # Backward pass through each layer (except output layer)\n",
        "        for i in range(self.num_of_layers - 2, 0, -1):\n",
        "            input_data, z, output = self.outputs[i - 1]\n",
        "            weight_matrix = self.weights[i]\n",
        "            delta = np.dot(weight_matrix.T, deltas[-1]) * self.sigmoid_derivative(z)\n",
        "            deltas.append(delta)\n",
        "\n",
        "        # Calculate derivatives of weights and biases\n",
        "        deltas.reverse()\n",
        "        for i in range(self.num_of_layers - 1):\n",
        "            input_data = self.outputs[i][0]\n",
        "            delta = deltas[i]\n",
        "            derivatives_w.append(np.dot(delta, input_data.T) / num_samples)\n",
        "            derivatives_b.append(np.mean(delta, axis=1, keepdims=True))\n",
        "\n",
        "        return derivatives_w, derivatives_b\n",
        "\n",
        "    def update_weights(self, derivatives_w, derivatives_b, learning_rate):\n",
        "        for i in range(self.num_of_layers - 1):\n",
        "            self.weights[i] -= learning_rate * derivatives_w[i]\n",
        "            self.biases[i] -= learning_rate * derivatives_b[i]\n",
        "\n",
        "    def train(self, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            y_pred = self.forward_pass(self.x)\n",
        "\n",
        "            # Backward pass\n",
        "            derivatives_w, derivatives_b = self.backward_pass(self.x, self.y)\n",
        "\n",
        "            # Update weights and biases\n",
        "            self.update_weights(derivatives_w, derivatives_b, learning_rate)\n",
        "\n",
        "            # Calculate and print loss\n",
        "            loss = self.mse_loss(self.y, y_pred)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WD3PyDR-ucep"
      },
      "outputs": [],
      "source": [
        "x_train=x_train.T.values\n",
        "y_train=y_train.T.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0faHZDxFuDbR"
      },
      "source": [
        " Step 7: Test the code with different architectures\n",
        " 1. Build NN with only 2 layers => 1 hidden layer and 1 output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVCrY5D7uDbV",
        "outputId": "0b7bd857-813b-49e0-844e-cc1e1ea04995"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[784, 20, 10]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-0524bd309bf3>:26: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Loss: 0.36463734331195236\n",
            "Epoch 2/100, Loss: 0.36259563121360494\n",
            "Epoch 3/100, Loss: 0.36056479924600915\n",
            "Epoch 4/100, Loss: 0.358544888915502\n",
            "Epoch 5/100, Loss: 0.35653597507622775\n",
            "Epoch 6/100, Loss: 0.35453816539770994\n",
            "Epoch 7/100, Loss: 0.3525515996740019\n",
            "Epoch 8/100, Loss: 0.3505764489653926\n",
            "Epoch 9/100, Loss: 0.34861291456356347\n",
            "Epoch 10/100, Loss: 0.346661226771345\n",
            "Epoch 11/100, Loss: 0.3447216434888474\n",
            "Epoch 12/100, Loss: 0.34279444859883634\n",
            "Epoch 13/100, Loss: 0.340879950145812\n",
            "Epoch 14/100, Loss: 0.33897847830541217\n",
            "Epoch 15/100, Loss: 0.3370903831434533\n",
            "Epoch 16/100, Loss: 0.3352160321672054\n",
            "Epoch 17/100, Loss: 0.33335580767525197\n",
            "Epoch 18/100, Loss: 0.3315101039164991\n",
            "Epoch 19/100, Loss: 0.32967932407339534\n",
            "Epoch 20/100, Loss: 0.3278638770891028\n",
            "Epoch 21/100, Loss: 0.3260641743630231\n",
            "Epoch 22/100, Loss: 0.3242806263435403\n",
            "Epoch 23/100, Loss: 0.32251363905090785\n",
            "Epoch 24/100, Loss: 0.32076361056666275\n",
            "Epoch 25/100, Loss: 0.31903092752864964\n",
            "Epoch 26/100, Loss: 0.31731596167249876\n",
            "Epoch 27/100, Loss: 0.3156190664611352\n",
            "Epoch 28/100, Loss: 0.31394057384354507\n",
            "Epoch 29/100, Loss: 0.31228079118254165\n",
            "Epoch 30/100, Loss: 0.3106399983887447\n",
            "Epoch 31/100, Loss: 0.30901844529446754\n",
            "Epoch 32/100, Loss: 0.307416349296826\n",
            "Epoch 33/100, Loss: 0.3058338932943399\n",
            "Epoch 34/100, Loss: 0.3042712239357333\n",
            "Epoch 35/100, Loss: 0.3027284501937957\n",
            "Epoch 36/100, Loss: 0.30120564227121255\n",
            "Epoch 37/100, Loss: 0.29970283083942983\n",
            "Epoch 38/100, Loss: 0.2982200066060252\n",
            "Epoch 39/100, Loss: 0.2967571202009097\n",
            "Epoch 40/100, Loss: 0.2953140823670541\n",
            "Epoch 41/100, Loss: 0.29389076443744544\n",
            "Epoch 42/100, Loss: 0.29248699907668235\n",
            "Epoch 43/100, Loss: 0.291102581263024\n",
            "Epoch 44/100, Loss: 0.2897372694848398\n",
            "Epoch 45/100, Loss: 0.288390787124224\n",
            "Epoch 46/100, Loss: 0.28706282400001176\n",
            "Epoch 47/100, Loss: 0.2857530380424861\n",
            "Epoch 48/100, Loss: 0.28446105707264746\n",
            "Epoch 49/100, Loss: 0.28318648065994434\n",
            "Epoch 50/100, Loss: 0.2819288820337521\n",
            "Epoch 51/100, Loss: 0.2806878100255788\n",
            "Epoch 52/100, Loss: 0.279462791020872\n",
            "Epoch 53/100, Loss: 0.2782533309013607\n",
            "Epoch 54/100, Loss: 0.2770589169609959\n",
            "Epoch 55/100, Loss: 0.2758790197807295\n",
            "Epoch 56/100, Loss: 0.27471309504952585\n",
            "Epoch 57/100, Loss: 0.27356058532110467\n",
            "Epoch 58/100, Loss: 0.272420921697926\n",
            "Epoch 59/100, Loss: 0.27129352543584684\n",
            "Epoch 60/100, Loss: 0.27017780946466124\n",
            "Epoch 61/100, Loss: 0.26907317982137235\n",
            "Epoch 62/100, Loss: 0.26797903699456366\n",
            "Epoch 63/100, Loss: 0.26689477717957605\n",
            "Epoch 64/100, Loss: 0.265819793445416\n",
            "Epoch 65/100, Loss: 0.26475347681538397\n",
            "Epoch 66/100, Loss: 0.2636952172643382\n",
            "Epoch 67/100, Loss: 0.26264440463631894\n",
            "Epoch 68/100, Loss: 0.2616004294869329\n",
            "Epoch 69/100, Loss: 0.2605626838554702\n",
            "Epoch 70/100, Loss: 0.25953056197219926\n",
            "Epoch 71/100, Loss: 0.25850346090666126\n",
            "Epoch 72/100, Loss: 0.2574807811630845\n",
            "Epoch 73/100, Loss: 0.25646192722926825\n",
            "Epoch 74/100, Loss: 0.25544630808544494\n",
            "Epoch 75/100, Loss: 0.2544333376797375\n",
            "Epoch 76/100, Loss: 0.2534224353768899\n",
            "Epoch 77/100, Loss: 0.25241302638696483\n",
            "Epoch 78/100, Loss: 0.2514045421806768\n",
            "Epoch 79/100, Loss: 0.2503964208979847\n",
            "Epoch 80/100, Loss: 0.24938810775647527\n",
            "Epoch 81/100, Loss: 0.24837905546596203\n",
            "Epoch 82/100, Loss: 0.24736872465558538\n",
            "Epoch 83/100, Loss: 0.2463565843195334\n",
            "Epoch 84/100, Loss: 0.24534211228730943\n",
            "Epoch 85/100, Loss: 0.2443247957242512\n",
            "Epoch 86/100, Loss: 0.24330413166774972\n",
            "Epoch 87/100, Loss: 0.2422796276043215\n",
            "Epoch 88/100, Loss: 0.24125080209236033\n",
            "Epoch 89/100, Loss: 0.2402171854350086\n",
            "Epoch 90/100, Loss: 0.239178320407161\n",
            "Epoch 91/100, Loss: 0.23813376304011935\n",
            "Epoch 92/100, Loss: 0.23708308346686482\n",
            "Epoch 93/100, Loss: 0.23602586683027896\n",
            "Epoch 94/100, Loss: 0.23496171425594334\n",
            "Epoch 95/100, Loss: 0.23389024389034543\n",
            "Epoch 96/100, Loss: 0.23281109200443548\n",
            "Epoch 97/100, Loss: 0.23172391416149038\n",
            "Epoch 98/100, Loss: 0.23062838644715236\n",
            "Epoch 99/100, Loss: 0.22952420675831806\n",
            "Epoch 100/100, Loss: 0.22841109614625696\n"
          ]
        }
      ],
      "source": [
        "nn1 = NeuralNetwork(x_train, y_train, 2, [20, 10])\n",
        "nn1.train(100, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W63JdBoCuDbY"
      },
      "source": [
        " 2. Build NN with 3 layers => 2 hidden layers\n",
        " \n",
        " Where # of neurons in first layer < # of neurons in second layer and 1 output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDSjQqdwuDbe",
        "outputId": "b9c5160b-1ebb-4235-a6dc-ff2d36d86cdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[784, 5, 20, 10]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-0524bd309bf3>:26: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Loss: 0.3168548041696878\n",
            "Epoch 2/100, Loss: 0.30596308735051736\n",
            "Epoch 3/100, Loss: 0.2952359466517837\n",
            "Epoch 4/100, Loss: 0.2847285519305092\n",
            "Epoch 5/100, Loss: 0.2744937632221013\n",
            "Epoch 6/100, Loss: 0.2645801984893538\n",
            "Epoch 7/100, Loss: 0.2550304690910622\n",
            "Epoch 8/100, Loss: 0.24587971973071965\n",
            "Epoch 9/100, Loss: 0.23715461086050305\n",
            "Epoch 10/100, Loss: 0.22887284183651674\n",
            "Epoch 11/100, Loss: 0.22104324298139705\n",
            "Epoch 12/100, Loss: 0.21366638690295928\n",
            "Epoch 13/100, Loss: 0.20673560638169008\n",
            "Epoch 14/100, Loss: 0.20023827123614024\n",
            "Epoch 15/100, Loss: 0.194157172023574\n",
            "Epoch 16/100, Loss: 0.18847187809539817\n",
            "Epoch 17/100, Loss: 0.18315997138871382\n",
            "Epoch 18/100, Loss: 0.1781980954145509\n",
            "Epoch 19/100, Loss: 0.17356279363752414\n",
            "Epoch 20/100, Loss: 0.16923113852487345\n",
            "Epoch 21/100, Loss: 0.16518117062748083\n",
            "Epoch 22/100, Loss: 0.16139217686574228\n",
            "Epoch 23/100, Loss: 0.15784484053005654\n",
            "Epoch 24/100, Loss: 0.15452129444481524\n",
            "Epoch 25/100, Loss: 0.15140510513931407\n",
            "Epoch 26/100, Loss: 0.14848121113221638\n",
            "Epoch 27/100, Loss: 0.14573583351573294\n",
            "Epoch 28/100, Loss: 0.14315637247800547\n",
            "Epoch 29/100, Loss: 0.14073129949858434\n",
            "Epoch 30/100, Loss: 0.1384500517779793\n",
            "Epoch 31/100, Loss: 0.13630293299501894\n",
            "Epoch 32/100, Loss: 0.13428102264662847\n",
            "Epoch 33/100, Loss: 0.13237609491282534\n",
            "Epoch 34/100, Loss: 0.13058054710154365\n",
            "Epoch 35/100, Loss: 0.12888733716698889\n",
            "Epoch 36/100, Loss: 0.12728992947757511\n",
            "Epoch 37/100, Loss: 0.1257822478650995\n",
            "Epoch 38/100, Loss: 0.12435863495959831\n",
            "Epoch 39/100, Loss: 0.12301381686113179\n",
            "Epoch 40/100, Loss: 0.12174287228858541\n",
            "Epoch 41/100, Loss: 0.12054120545386635\n",
            "Epoch 42/100, Loss: 0.11940452202245722\n",
            "Epoch 43/100, Loss: 0.11832880762873893\n",
            "Epoch 44/100, Loss: 0.11731030851153659\n",
            "Epoch 45/100, Loss: 0.11634551391959194\n",
            "Epoch 46/100, Loss: 0.11543114000764987\n",
            "Epoch 47/100, Loss: 0.1145641150022262\n",
            "Epoch 48/100, Loss: 0.1137415654631925\n",
            "Epoch 49/100, Loss: 0.11296080350461797\n",
            "Epoch 50/100, Loss: 0.1122193148674337\n",
            "Epoch 51/100, Loss: 0.1115147477589214\n",
            "Epoch 52/100, Loss: 0.11084490239109755\n",
            "Epoch 53/100, Loss: 0.11020772116290511\n",
            "Epoch 54/100, Loss: 0.10960127944067968\n",
            "Epoch 55/100, Loss: 0.1090237768984043\n",
            "Epoch 56/100, Loss: 0.1084735293844235\n",
            "Epoch 57/100, Loss: 0.10794896128503788\n",
            "Epoch 58/100, Loss: 0.1074485983581354\n",
            "Epoch 59/100, Loss: 0.10697106101201662\n",
            "Epoch 60/100, Loss: 0.10651505800606523\n",
            "Epoch 61/100, Loss: 0.10607938055106744\n",
            "Epoch 62/100, Loss: 0.10566289678790519\n",
            "Epoch 63/100, Loss: 0.10526454662414535\n",
            "Epoch 64/100, Loss: 0.10488333690875017\n",
            "Epoch 65/100, Loss: 0.10451833692581965\n",
            "Epoch 66/100, Loss: 0.10416867418893261\n",
            "Epoch 67/100, Loss: 0.1038335305183306\n",
            "Epoch 68/100, Loss: 0.1035121383838656\n",
            "Epoch 69/100, Loss: 0.10320377749733022\n",
            "Epoch 70/100, Loss: 0.10290777163849966\n",
            "Epoch 71/100, Loss: 0.10262348569993156\n",
            "Epoch 72/100, Loss: 0.1023503229362949\n",
            "Epoch 73/100, Loss: 0.10208772240472497\n",
            "Epoch 74/100, Loss: 0.10183515658341796\n",
            "Epoch 75/100, Loss: 0.10159212915638974\n",
            "Epoch 76/100, Loss: 0.10135817295301783\n",
            "Epoch 77/100, Loss: 0.10113284803166296\n",
            "Epoch 78/100, Loss: 0.1009157398973203\n",
            "Epoch 79/100, Loss: 0.10070645784388446\n",
            "Epoch 80/100, Loss: 0.10050463341221695\n",
            "Epoch 81/100, Loss: 0.10030991895578231\n",
            "Epoch 82/100, Loss: 0.10012198630617374\n",
            "Epoch 83/100, Loss: 0.0999405255313659\n",
            "Epoch 84/100, Loss: 0.09976524378003157\n",
            "Epoch 85/100, Loss: 0.09959586420572097\n",
            "Epoch 86/100, Loss: 0.0994321249651418\n",
            "Epoch 87/100, Loss: 0.09927377828518846\n",
            "Epoch 88/100, Loss: 0.099120589593754\n",
            "Epoch 89/100, Loss: 0.09897233670971786\n",
            "Epoch 90/100, Loss: 0.09882880908783782\n",
            "Epoch 91/100, Loss: 0.09868980711458752\n",
            "Epoch 92/100, Loss: 0.09855514145127219\n",
            "Epoch 93/100, Loss: 0.09842463242102613\n",
            "Epoch 94/100, Loss: 0.09829810943654474\n",
            "Epoch 95/100, Loss: 0.09817541046564099\n",
            "Epoch 96/100, Loss: 0.09805638153192797\n",
            "Epoch 97/100, Loss: 0.09794087624813291\n",
            "Epoch 98/100, Loss: 0.09782875537973158\n",
            "Epoch 99/100, Loss: 0.09771988643676376\n",
            "Epoch 100/100, Loss: 0.09761414329185138\n"
          ]
        }
      ],
      "source": [
        "nn2 = NeuralNetwork(x_train, y_train, 3, [5, 20, 10])\n",
        "nn2.train(100, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaVoeg6xuDbg"
      },
      "source": [
        " 3. Build NN with 4 layers => 3 hidden layers\n",
        " \n",
        " Where # of neurons in first layer > # of neurons in second layer and 1 output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOV3K6SHuDbi",
        "outputId": "05fa6859-b565-4393-b475-41d845cd4132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[784, 20, 5, 10]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-0524bd309bf3>:26: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Loss: 0.3497074767711691\n",
            "Epoch 2/100, Loss: 0.3464961016106895\n",
            "Epoch 3/100, Loss: 0.3433015646664084\n",
            "Epoch 4/100, Loss: 0.34012703588436927\n",
            "Epoch 5/100, Loss: 0.3369755543847343\n",
            "Epoch 6/100, Loss: 0.33385000177391927\n",
            "Epoch 7/100, Loss: 0.3307530786712348\n",
            "Epoch 8/100, Loss: 0.3276872848046799\n",
            "Epoch 9/100, Loss: 0.3246549029158458\n",
            "Epoch 10/100, Loss: 0.32165798659735667\n",
            "Epoch 11/100, Loss: 0.31869835207333025\n",
            "Epoch 12/100, Loss: 0.3157775738287635\n",
            "Epoch 13/100, Loss: 0.312896983901478\n",
            "Epoch 14/100, Loss: 0.3100576745730626\n",
            "Epoch 15/100, Loss: 0.30726050413486433\n",
            "Epoch 16/100, Loss: 0.30450610536203526\n",
            "Epoch 17/100, Loss: 0.30179489630253986\n",
            "Epoch 18/100, Loss: 0.2991270929776163\n",
            "Epoch 19/100, Loss: 0.2965027235936041\n",
            "Epoch 20/100, Loss: 0.2939216438800274\n",
            "Epoch 21/100, Loss: 0.2913835531929151\n",
            "Epoch 22/100, Loss: 0.28888801105306366\n",
            "Epoch 23/100, Loss: 0.28643445382397514\n",
            "Epoch 24/100, Loss: 0.2840222112714326\n",
            "Epoch 25/100, Loss: 0.2816505227843317\n",
            "Epoch 26/100, Loss: 0.27931855307300646\n",
            "Epoch 27/100, Loss: 0.27702540719581153\n",
            "Epoch 28/100, Loss: 0.27477014479635214\n",
            "Epoch 29/100, Loss: 0.2725517934620502\n",
            "Epoch 30/100, Loss: 0.2703693611394713\n",
            "Epoch 31/100, Loss: 0.26822184756302503\n",
            "Epoch 32/100, Loss: 0.26610825467143145\n",
            "Epoch 33/100, Loss: 0.2640275960010017\n",
            "Epoch 34/100, Loss: 0.26197890505666827\n",
            "Epoch 35/100, Loss: 0.2599612426711915\n",
            "Epoch 36/100, Loss: 0.25797370337049214\n",
            "Epoch 37/100, Loss: 0.25601542076900785\n",
            "Epoch 38/100, Loss: 0.25408557202370186\n",
            "Epoch 39/100, Loss: 0.2521833813792343\n",
            "Epoch 40/100, Loss: 0.2503081228400835\n",
            "Epoch 41/100, Loss: 0.24845912200835715\n",
            "Epoch 42/100, Loss: 0.24663575712880376\n",
            "Epoch 43/100, Loss: 0.24483745938527313\n",
            "Epoch 44/100, Loss: 0.24306371249564135\n",
            "Epoch 45/100, Loss: 0.24131405165502298\n",
            "Epoch 46/100, Loss: 0.23958806187995085\n",
            "Epoch 47/100, Loss: 0.2378853758089998\n",
            "Epoch 48/100, Loss: 0.23620567101802661\n",
            "Epoch 49/100, Loss: 0.234548666910641\n",
            "Epoch 50/100, Loss: 0.23291412124660696\n",
            "Epoch 51/100, Loss: 0.23130182637246688\n",
            "Epoch 52/100, Loss: 0.22971160521965525\n",
            "Epoch 53/100, Loss: 0.2281433071356191\n",
            "Epoch 54/100, Loss: 0.2265968036128999\n",
            "Epoch 55/100, Loss: 0.2250719839796987\n",
            "Epoch 56/100, Loss: 0.22356875111310398\n",
            "Epoch 57/100, Loss: 0.2220870172329361\n",
            "Epoch 58/100, Loss: 0.2206266998300713\n",
            "Epoch 59/100, Loss: 0.21918771777824284\n",
            "Epoch 60/100, Loss: 0.21776998767277525\n",
            "Epoch 61/100, Loss: 0.2163734204336178\n",
            "Epoch 62/100, Loss: 0.21499791820355332\n",
            "Epoch 63/100, Loss: 0.213643371565738\n",
            "Epoch 64/100, Loss: 0.2123096570979273\n",
            "Epoch 65/100, Loss: 0.21099663527403476\n",
            "Epoch 66/100, Loss: 0.20970414871721438\n",
            "Epoch 67/100, Loss: 0.2084320208025732\n",
            "Epoch 68/100, Loss: 0.20718005460205227\n",
            "Epoch 69/100, Loss: 0.20594803215903898\n",
            "Epoch 70/100, Loss: 0.20473571407597269\n",
            "Epoch 71/100, Loss: 0.2035428393946264\n",
            "Epoch 72/100, Loss: 0.2023691257459003\n",
            "Epoch 73/100, Loss: 0.20121426974385884\n",
            "Epoch 74/100, Loss: 0.20007794759734662\n",
            "Epoch 75/100, Loss: 0.19895981591179313\n",
            "Epoch 76/100, Loss: 0.19785951265370516\n",
            "Epoch 77/100, Loss: 0.19677665825077867\n",
            "Epoch 78/100, Loss: 0.1957108568014693\n",
            "Epoch 79/100, Loss: 0.19466169736915961\n",
            "Epoch 80/100, Loss: 0.19362875533767457\n",
            "Epoch 81/100, Loss: 0.1926115938067517\n",
            "Epoch 82/100, Loss: 0.19160976500808888\n",
            "Epoch 83/100, Loss: 0.19062281172471324\n",
            "Epoch 84/100, Loss: 0.1896502686985748\n",
            "Epoch 85/100, Loss: 0.18869166401341217\n",
            "Epoch 86/100, Loss: 0.18774652044204057\n",
            "Epoch 87/100, Loss: 0.18681435674920766\n",
            "Epoch 88/100, Loss: 0.1858946889430591\n",
            "Epoch 89/100, Loss: 0.18498703147000292\n",
            "Epoch 90/100, Loss: 0.1840908983493623\n",
            "Epoch 91/100, Loss: 0.18320580424565078\n",
            "Epoch 92/100, Loss: 0.18233126547758094\n",
            "Epoch 93/100, Loss: 0.18146680096403933\n",
            "Epoch 94/100, Loss: 0.18061193310822138\n",
            "Epoch 95/100, Loss: 0.17976618862193325\n",
            "Epoch 96/100, Loss: 0.17892909929274295\n",
            "Epoch 97/100, Loss: 0.17810020269719828\n",
            "Epoch 98/100, Loss: 0.1772790428637567\n",
            "Epoch 99/100, Loss: 0.17646517088937777\n",
            "Epoch 100/100, Loss: 0.175658145513946\n"
          ]
        }
      ],
      "source": [
        "nn3 = NeuralNetwork(x_train, y_train, 3, [20,5, 10])\n",
        "nn3.train(100, 0.1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
