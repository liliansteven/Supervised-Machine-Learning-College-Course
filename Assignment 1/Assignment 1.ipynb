{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from matplotlib import pyplot\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 28, 28)\n",
      "Y_train: (60000,)\n",
      "X_test:  (10000, 28, 28)\n",
      "Y_test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "#loading\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    " \n",
    "#shape of dataset\n",
    "print('X_train: ' + str(X_train.shape))\n",
    "print('Y_train: ' + str(Y_train.shape))\n",
    "print('X_test:  '  + str(X_test.shape))\n",
    "print('Y_test:  '  + str(Y_test.shape))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting\n",
    "for i in range(1):  \n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    pyplot.imshow(X_train[i], cmap=pyplot.get_cmap('gray'))\n",
    "    pyplot.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Subset your data to use only class 0 and class1 for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_indx = np.array([])\n",
    "Test_indx =np.array([])\n",
    "\n",
    "for i in range(len(Y_train)-1):\n",
    "    if(Y_train[i]==0 or Y_train[i]==1):\n",
    "        Train_indx = np.append(Train_indx, i )\n",
    "\n",
    "for i in range(len(Y_test)-1):\n",
    "    if(Y_test[i]==0 or Y_test[i]==1):\n",
    "        Test_indx = np.append(Test_indx, i )\n",
    "\n",
    "Train_indx = Train_indx.astype(int)\n",
    "Test_indx = Test_indx.astype(int)\n",
    "\n",
    "X_train = X_train[Train_indx]\n",
    "X_test = X_test[Test_indx]\n",
    "Y_train = Y_train[Train_indx]\n",
    "Y_test = Y_test[Test_indx]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Standardize your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train = np.mean(X_train, axis=0)\n",
    "std_train = np.std(X_train, axis=0)\n",
    "std_train[std_train == 0] = 1e-9\n",
    "X_train = (X_train - mean_train) / std_train\n",
    "\n",
    "mean_test = np.mean(X_test, axis=0)\n",
    "std_test = np.std(X_test, axis=0)\n",
    "std_test[std_test == 0] = 1e-9\n",
    "X_test = (X_test - mean_test) / std_test\n",
    "\n",
    "X_data = np.concatenate((X_train, X_test), axis=0)\n",
    "y_data = np.concatenate((Y_train, Y_test), axis=0)\n",
    "\n",
    "X_data = X_data.reshape(X_data.shape[0],-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Divide data into training and validation set using 10-fold cross validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into k folds\n",
    "def k_folds_split(X, y, k):\n",
    "    n_samples = len(X)\n",
    "    fold_size = n_samples // k\n",
    "    X_folds = []\n",
    "    y_folds = []\n",
    "    \n",
    "    # Shuffle the data\n",
    "    permuted_indices = np.random.permutation(n_samples)\n",
    "    X_shuffled = X[permuted_indices]\n",
    "    y_shuffled = y[permuted_indices]\n",
    "    \n",
    "    # Split the data into k folds\n",
    "    for i in range(k):\n",
    "        X_fold = X_shuffled[i * fold_size : (i + 1) * fold_size]\n",
    "        y_fold = y_shuffled[i * fold_size : (i + 1) * fold_size]\n",
    "        X_folds.append(X_fold)\n",
    "        y_folds.append(y_fold)\n",
    "    \n",
    "    return X_folds, y_folds\n",
    "\n",
    "# Train the model using k-fold cross validation\n",
    "def k_fold_cross_validation(X, y, k, model):\n",
    "    X_folds, y_folds = k_folds_split(X, y, k)\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        X_train = np.concatenate([X_folds[j] for j in range(k) if j != i])\n",
    "        y_train = np.concatenate([y_folds[j] for j in range(k) if j != i])\n",
    "        X_val = X_folds[i]\n",
    "        y_val = y_folds[i]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        sc = model.score(y_val, y_pred)\n",
    "        scores.append(sc)\n",
    "        \n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Implement Logistic Regression with different values for learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression():\n",
    "    \n",
    "    \n",
    "    # declaring learning rate & number of iterations (Hyperparametes)\n",
    "    def __init__(self, learning_rate, no_of_iterations):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.no_of_iterations = no_of_iterations\n",
    "        \n",
    "        \n",
    "    # fit function to train the model with dataset\n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        # number of data points in the dataset (number of rows)  -->  m\n",
    "        # number of input features in the dataset (number of columns)  --> n\n",
    "        self.m, self.n = X.shape\n",
    "        \n",
    "        #initiating weight & bias value\n",
    "        self.w = np.zeros(self.n)\n",
    "        self.b = 0\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "        # implementing Gradient Descent for Optimization\n",
    "        for i in range(self.no_of_iterations):\n",
    "            self.update_weights()       \n",
    "        \n",
    "    def update_weights(self):\n",
    "        # Y_hat formula (sigmoid function)\n",
    "        Y_hat = 1 / (1 + np.exp( - (self.X.dot(self.w) + self.b ) ))    \n",
    "        \n",
    "        # derivaties\n",
    "        dw = (1/self.m)*np.dot(self.X.T, (Y_hat - self.Y))\n",
    "        \n",
    "        db = (1/self.m)*np.sum(Y_hat - self.Y)\n",
    "        \n",
    "        # updating the weights & bias using gradient descent\n",
    "        self.w = self.w - self.learning_rate * dw\n",
    "        \n",
    "        self.b = self.b - self.learning_rate * db\n",
    "      \n",
    "    # Sigmoid Equation & Decision Boundary\n",
    "    def predict(self, X):\n",
    "        Y_pred = 1 / (1 + np.exp( - (X.dot(self.w) + self.b ) ))\n",
    "        Y_pred = np.where( Y_pred > 0.5, 1, 0)\n",
    "        return Y_pred\n",
    "    \n",
    "\n",
    "    def score(self,y,y_hat):\n",
    "        tp,tn,fp,fn = 0,0,0,0\n",
    "        for i in range(len(y)):\n",
    "            if y[i] == 1 and y_hat[i] == 1:\n",
    "                tp += 1\n",
    "            elif y[i] == 1 and y_hat[i] == 0:\n",
    "                fn += 1\n",
    "            elif y[i] == 0 and y_hat[i] == 1:\n",
    "                fp += 1\n",
    "            elif y[i] == 0 and y_hat[i] == 0:\n",
    "                tn += 1\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        score = 2*precision*recall/(precision+recall)\n",
    "        return score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Report difference accuracy for the different learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score of Learning rate 1 : 0.9989276078075375\n",
      "Average score of Learning rate 0.1 : 0.9989203142972262\n",
      "Average score of Learning rate 0.01 : 0.9986142806799736\n",
      "Average score of Learning rate 0.001 : 0.9968422376831365\n",
      "Average score of Learning rate 0.0001 : 0.9950641933522848\n"
     ]
    }
   ],
   "source": [
    "values_of_learning_rate = [1 , 0.1 , 0.01 , 0.001 , 0.0001]\n",
    "\n",
    "for curr_val in values_of_learning_rate : \n",
    "    model = Logistic_Regression(curr_val , 1000)\n",
    "    scores = k_fold_cross_validation(X_data,y_data,10,model)\n",
    "    print(f\"Average score of Learning rate {curr_val} :\" , scores)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
