{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "df846703",
      "metadata": {
        "id": "df846703"
      },
      "source": [
        "## Implementing some libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "c3ea5773",
      "metadata": {
        "id": "c3ea5773"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "from matplotlib import pyplot\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fcfb50c",
      "metadata": {
        "id": "8fcfb50c"
      },
      "source": [
        "## 1. Load MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "ce1d88cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce1d88cd",
        "outputId": "3984bd84-d522-4e5a-fe79-74356f52f541"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train: (60000, 28, 28)\n",
            "Y_train: (60000,)\n",
            "X_test:  (10000, 28, 28)\n",
            "Y_test:  (10000,)\n"
          ]
        }
      ],
      "source": [
        "#loading\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        " \n",
        "#shape of dataset\n",
        "print('X_train: ' + str(X_train.shape))\n",
        "print('Y_train: ' + str(Y_train.shape))\n",
        "print('X_test:  '  + str(X_test.shape))\n",
        "print('Y_test:  '  + str(Y_test.shape))\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "cc2a56fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "cc2a56fd",
        "outputId": "b5790687-81fa-42a7-ea33-e8439ec9fddb"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#plotting\n",
        "for i in range(1):  \n",
        "    pyplot.subplot(330 + 1 + i)\n",
        "    pyplot.imshow(X_train[i], cmap=pyplot.get_cmap('gray'))\n",
        "    pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45d22c18",
      "metadata": {
        "id": "45d22c18"
      },
      "source": [
        "## 2. Subset your data to use only class 0 and class1 for the next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "ed45affa",
      "metadata": {
        "id": "ed45affa"
      },
      "outputs": [],
      "source": [
        "Train_indx = np.array([])\n",
        "Test_indx =np.array([])\n",
        "\n",
        "for i in range(len(Y_train)-1):\n",
        "    if(Y_train[i]==0 or Y_train[i]==1):\n",
        "        Train_indx = np.append(Train_indx, i )\n",
        "\n",
        "for i in range(len(Y_test)-1):\n",
        "    if(Y_test[i]==0 or Y_test[i]==1):\n",
        "        Test_indx = np.append(Test_indx, i )\n",
        "\n",
        "Train_indx = Train_indx.astype(int)\n",
        "Test_indx = Test_indx.astype(int)\n",
        "\n",
        "X_train = X_train[Train_indx]\n",
        "X_test = X_test[Test_indx]\n",
        "Y_train = Y_train[Train_indx]\n",
        "Y_test = Y_test[Test_indx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f339101c",
      "metadata": {
        "id": "f339101c"
      },
      "source": [
        "## 3. Standardize your dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "8e24f7d1",
      "metadata": {
        "id": "8e24f7d1"
      },
      "outputs": [],
      "source": [
        "mean_train = np.mean(X_train, axis=0)\n",
        "std_train = np.std(X_train, axis=0)\n",
        "std_train[std_train == 0] = 1e-9\n",
        "X_train = (X_train - mean_train) / std_train\n",
        "\n",
        "mean_test = np.mean(X_test, axis=0)\n",
        "std_test = np.std(X_test, axis=0)\n",
        "std_test[std_test == 0] = 1e-9\n",
        "X_test = (X_test - mean_test) / std_test\n",
        "\n",
        "X_data = np.concatenate((X_train, X_test), axis=0)\n",
        "y_data = np.concatenate((Y_train, Y_test), axis=0)\n",
        "\n",
        "X_data = X_data.reshape(X_data.shape[0],-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42b61ebf",
      "metadata": {
        "id": "42b61ebf"
      },
      "source": [
        "## 4. Divide data into training and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "392f21b1",
      "metadata": {
        "id": "392f21b1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "875c4b00",
      "metadata": {
        "id": "875c4b00"
      },
      "source": [
        "## 5. Implement Logistic Regression\n",
        "6. Use L1 regularization with gradient descent optimizer. Try 2 values for lambda\n",
        "\n",
        "7. Use mini-batch gradient descent optimizer. Try multiple batches (at least 2)\n",
        "8. Use RMS Prob optimizer and Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "de404869",
      "metadata": {
        "id": "de404869"
      },
      "outputs": [],
      "source": [
        "#As mentioned before, logistic regression is a method for classification. Hence, it is expected to output a value which is a probability. \n",
        "#Because of that reason, unlike linear regression, an activation function called \"sigmoid function\" is employed for the output\n",
        "class LogisticRegression():\n",
        "    # optimizer : is the type of optimizer we will use , we just can use any of the following \n",
        "    # optimizers by passing their name as ARG to the class (gd , mini_batch , rmsprop , adam )\n",
        "    # batch_size : will use for mini_batch classifier \n",
        "    # l1_penalty : its the lamdaa paramter for L1 regulization \n",
        "    def __init__(self, learning_rate=0.00001, num_iterations=1000, optimizer='gd', batch_size=32, l1_penalty=0):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.optimizer = optimizer\n",
        "        self.batch_size = batch_size\n",
        "        self.l1_penalty = l1_penalty\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        m, n = X.shape\n",
        "        self.weights = np.zeros(n)\n",
        "        self.bias = 0\n",
        "        ##Instead of doing many classes, \n",
        "        ##the difference between them is just the way of fitting, and therefore I collected all of that in one class\n",
        "        for i in range(self.num_iterations):\n",
        "            if self.optimizer == 'gd':\n",
        "                gradients = self.compute_gradients(X, y)\n",
        "                self.weights -= self.learning_rate * gradients['dw']\n",
        "                self.bias -= self.learning_rate * gradients['db']\n",
        "            elif self.optimizer == 'mini_batch':\n",
        "                #dividing the data training into batches and each of them has the given size \n",
        "                batch_indices = np.random.choice(m, self.batch_size, replace=False)\n",
        "                X_batch = X[batch_indices]\n",
        "                y_batch = y[batch_indices]\n",
        "                gradients = self.compute_gradients(X_batch, y_batch)\n",
        "                self.weights -= self.learning_rate * gradients['dw']\n",
        "                self.bias -= self.learning_rate * gradients['db']\n",
        "            elif self.optimizer == 'rmsprop':\n",
        "                gradients = self.compute_gradients(X, y)\n",
        "                self.rmsprop_update(gradients)\n",
        "            elif self.optimizer == 'adam':\n",
        "                gradients = self.compute_gradients(X, y)\n",
        "                self.adam_update(gradients, i)\n",
        "\n",
        "    def compute_gradients(self, X, y):\n",
        "        m = X.shape[0]\n",
        "        y_pred = self.predict_proba(X)\n",
        "        error = y_pred - y\n",
        "\n",
        "        dw = (1 / m) * X.T.dot(error) + (self.l1_penalty / m) * np.sign(self.weights)\n",
        "        db = np.mean(error)\n",
        "\n",
        "        gradients = {'dw': dw, 'db': db}\n",
        "        return gradients\n",
        "\n",
        "    def rmsprop_update(self, gradients, epsilon=1e-8, decay_rate=0.9):\n",
        "        \n",
        "        # Initialize squared_gradients dictionary if it doesn't exist\n",
        "        if not hasattr(self, 'squared_gradients'):\n",
        "            self.squared_gradients = {}\n",
        "            self.squared_gradients['dw'] = np.zeros_like(gradients['dw'])\n",
        "            self.squared_gradients['db'] = np.zeros_like(gradients['db'])\n",
        "\n",
        "        # Update squared gradients using decay rate and current gradients\n",
        "        self.squared_gradients['dw'] = decay_rate * self.squared_gradients['dw'] + (1 - decay_rate) * np.square(gradients['dw'])\n",
        "        self.squared_gradients['db'] = decay_rate * self.squared_gradients['db'] + (1 - decay_rate) * np.square(gradients['db'])\n",
        "\n",
        "        # Update weights and bias using RMSProp update rule\n",
        "        self.weights -= self.learning_rate * gradients['dw'] / (np.sqrt(self.squared_gradients['dw']) + epsilon)\n",
        "        self.bias -= self.learning_rate * gradients['db'] / (np.sqrt(self.squared_gradients['db']) + epsilon)\n",
        "\n",
        "\n",
        "    def adam_update(self, gradients, iteration, epsilon=1e-8, beta1=0.9, beta2=0.999):\n",
        "        if not hasattr(self, 'm'):\n",
        "            self.m = {}\n",
        "            self.m['dw'] = np.zeros_like(gradients['dw'])\n",
        "            self.m['db'] = np.zeros_like(gradients['db'])\n",
        "    \n",
        "            self.v = {}\n",
        "            self.v['dw'] = np.zeros_like(gradients['dw'])\n",
        "            self.v['db'] = np.zeros_like(gradients['db'])\n",
        "    \n",
        "        self.m['dw'] = beta1 * self.m['dw'] + (1 - beta1) * gradients['dw']\n",
        "        self.m['db'] = beta1 * self.m['db'] + (1 - beta1) * gradients['db']\n",
        "    \n",
        "        self.v['dw'] = beta2 * self.v['dw'] + (1 - beta2) * np.square(gradients['dw'])  \n",
        "        self.v['db'] = beta2 * self.v['db'] + (1 - beta2) * np.square(gradients['db'])\n",
        "    \n",
        "        m_corrected_dw = self.m['dw'] / (1 - np.power(beta1, iteration + 1))\n",
        "        m_corrected_db = self.m['db'] / (1 - np.power(beta1, iteration + 1))\n",
        "    \n",
        "        v_corrected_dw = self.v['dw'] / (1 - np.power(beta2, iteration + 1))\n",
        "        v_corrected_db = self.v['db'] / (1 - np.power(beta2, iteration + 1))\n",
        "    \n",
        "        self.weights -= self.learning_rate * m_corrected_dw / (np.sqrt(v_corrected_dw) + epsilon)\n",
        "        self.bias -= self.learning_rate * m_corrected_db / (np.sqrt(v_corrected_db) + epsilon)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        z = X.dot(self.weights) + self.bias\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        probabilities = self.predict_proba(X)\n",
        "        return np.where(probabilities >= threshold, 1, 0)\n",
        "    \n",
        "    def accuracy(self, y_true, y_pred):\n",
        "        correct = np.sum(y_true == y_pred)\n",
        "        total = len(y_true)\n",
        "        return correct / total\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc32d003",
      "metadata": {
        "id": "fc32d003"
      },
      "source": [
        "## Q9. report the Accuracies of the abone classes and write a conclusion for each case explained the behind reasons :-"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd1d0d78",
      "metadata": {
        "id": "dd1d0d78"
      },
      "source": [
        "# (1) For Nurmal GD with logistic regression without any optimizations :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "0c9fba89",
      "metadata": {
        "id": "0c9fba89"
      },
      "outputs": [],
      "source": [
        "model_gd = LogisticRegression(optimizer='gd')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "dcbf66bf",
      "metadata": {
        "id": "dcbf66bf"
      },
      "outputs": [],
      "source": [
        "# fiting the model first \n",
        "model_gd.fit(X_train , y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "662b339a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "662b339a",
        "outputId": "b289587e-f098-4b8b-c82b-e3712c275ae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the Accuracy of the logistic Regression with Gredient Descent without any regulization is : 0.9942489851150202\n"
          ]
        }
      ],
      "source": [
        "y_pred = model_gd.predict(X_test)\n",
        "print(f\"the Accuracy of the logistic Regression with Gredient Descent without any regulization is : {model_gd.accuracy(y_test , y_pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R8fwv9jOhjsu",
      "metadata": {
        "id": "R8fwv9jOhjsu"
      },
      "source": [
        "#### conclusion of Nurmal GD with logistic regression without any optimizations : With an accuracy of nearly 99.42%, the model is able to correctly classify the majority of the instances in the dataset. This high accuracy suggests that the model has learned the underlying patterns and relationships in the data, making it effective for making predictions. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4b9ead3e",
      "metadata": {
        "id": "4b9ead3e"
      },
      "source": [
        "# (2) Use L1 regularization with gradient descent optimizer. Try 2 values for lambda :- "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "148098d3",
      "metadata": {
        "id": "148098d3"
      },
      "source": [
        "### (1) L1 regularization with gradient descent optimizer with lambda = 0.001 :- "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "99544242",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99544242",
        "outputId": "7bb94ae8-d553-4a65-fd03-7b6d3df17929"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the Accuracy of the logistic Regression with L1 regulization With lamda equal 0.001 is : 0.9942489851150202\n"
          ]
        }
      ],
      "source": [
        "model_mini_L1_1 = LogisticRegression(optimizer='gd' , l1_penalty = 0.001) ## or 0.01\n",
        "model_mini_L1_1.fit(X_train , y_train)\n",
        "y_pred = model_mini_L1_1.predict(X_test)\n",
        "print(f\"the Accuracy of the logistic Regression with L1 regulization With lamda equal {0.001} is : {model_mini_L1_1.accuracy(y_test , y_pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b407385",
      "metadata": {
        "id": "4b407385"
      },
      "source": [
        "### (2) L1 regularization with gradient descent optimizer with lambda =  1 :-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "02c40041",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02c40041",
        "outputId": "29d3a3e2-59b3-4784-e296-6aab93a78f6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the Accuracy of the logistic Regression with L1 regulization With lamda equal 1 is : 0.9942489851150202\n"
          ]
        }
      ],
      "source": [
        "model_mini_L1_2 = LogisticRegression(optimizer='gd' , l1_penalty = 1)\n",
        "model_mini_L1_2.fit(X_train , y_train)\n",
        "y_pred = model_mini_L1_2.predict(X_test)\n",
        "print(f\"the Accuracy of the logistic Regression with L1 regulization With lamda equal {1} is : {model_mini_L1_2.accuracy(y_test , y_pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0fe0e56",
      "metadata": {
        "id": "a0fe0e56"
      },
      "source": [
        "### (3) L1 regularization with gradient descent optimizer with lambda = 100 :-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "208da960",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "208da960",
        "outputId": "bb4f47df-bc3b-4b7c-e5c9-5270d260f5b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the Accuracy of the logistic Regression with L1 regulization With lamda equal :  100 is  : 0.9949255751014885\n"
          ]
        }
      ],
      "source": [
        "model_mini_L1_3 = LogisticRegression(optimizer='gd' , l1_penalty = 100) \n",
        "model_mini_L1_3.fit(X_train , y_train)\n",
        "y_pred = model_mini_L1_3.predict(X_test)\n",
        "print(f\"the Accuracy of the logistic Regression with L1 regulization With lamda equal :  {100} is  : {model_mini_L1_3.accuracy(y_test , y_pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26fdb563",
      "metadata": {
        "id": "26fdb563"
      },
      "source": [
        "(1) Small values of lambda (e.g., 0.01, 0.001): Small values of lambda result in less regularization, allowing the model to focus more on fitting the training data. This can be useful when you have a large number of features and believe that most of them are relevant.\n",
        "\n",
        "(2) Moderate values of lambda (e.g., 0.1, 1): Moderate values of lambda strike a balance between fitting the data and reducing overfitting. These values can help in preventing the model from becoming too sensitive to individual features and capturing more general patterns in the data.\n",
        "\n",
        "(3) Large values of lambda (e.g., 10, 100): Large values of lambda increase the amount of regularization, which can be useful when you have many irrelevant features or when you want to simplify the model. These values penalize the magnitude of the coefficients more strongly, leading to sparser solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ikSr8fVziLzn",
      "metadata": {
        "id": "ikSr8fVziLzn"
      },
      "source": [
        "So The L1 regularization helps in reducing overfitting and improving generalization.\n",
        "Because of the high accuracy of 0.9942, it indicates that the model is performing exceptionally well on the given dataset. Incorporating L1 regularization has several implications:\n",
        "\n",
        "Feature Selection: L1 regularization promotes sparsity by setting less important feature coefficients to zero. This aids in identifying and prioritizing the most relevant features for the target variable.\n",
        "\n",
        "Model Simplicity: L1 regularization leads to a simpler model by discarding irrelevant or redundant features. Simpler models are less prone to overfitting and can generalize better to unseen data.\n",
        "\n",
        "Increased Interpretability: Sparse models resulting from L1 regularization are easier to interpret as non-zero coefficients directly indicate feature importance and influence on the target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QhQk841xdfBe",
      "metadata": {
        "id": "QhQk841xdfBe"
      },
      "source": [
        "### 3_Logistic Regression With Mini Batch Gradient Descent with different  batch size :-"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ed6589",
      "metadata": {
        "id": "51ed6589"
      },
      "source": [
        "### Logistic Regression With Mini Batch Gradient Descent with batch size = 750 as (60000/750 = 80 batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "37e543a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37e543a6",
        "outputId": "f30beb56-b7ed-4060-b193-d96491e6129f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the Accuracy of the logistic Regression with Mini Batch Gradient Descent with batch_size equal 750 is : 0.9942489851150202\n"
          ]
        }
      ],
      "source": [
        "model_mini_batch_1 = LogisticRegression(optimizer='mini_batch' , batch_size= 750)\n",
        "model_mini_batch_1.fit(X_train , y_train)\n",
        "y_pred = model_mini_batch_1.predict(X_test)\n",
        "print(f\"the Accuracy of the logistic Regression with Mini Batch Gradient Descent with batch_size equal {750} is : {model_mini_batch_1.accuracy(y_test , y_pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46f9a8e2",
      "metadata": {
        "id": "46f9a8e2"
      },
      "source": [
        "### Logistic Regression With Mini Batch Gradient Descent with batch size = 600 as (60000/600 = 100 batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "c0d5d428",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0d5d428",
        "outputId": "5129c3ae-4657-4bdd-94cd-264b338fb25a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the Accuracy of the logistic Regression with Mini Batch Gradient Descent with batch_size equal 600 is : 0.9942489851150202\n"
          ]
        }
      ],
      "source": [
        "model_mini_batch_2 = LogisticRegression(optimizer='mini_batch' , batch_size= 600)\n",
        "model_mini_batch_2.fit(X_train , y_train)\n",
        "y_pred = model_mini_batch_2.predict(X_test)\n",
        "print(f\"the Accuracy of the logistic Regression with Mini Batch Gradient Descent with batch_size equal {600} is : {model_mini_batch_2.accuracy(y_test , y_pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb1ef761",
      "metadata": {
        "id": "eb1ef761"
      },
      "source": [
        "### Logistic Regression With Mini Batch Gradient Descent with batch size = 800 as (60000/800 = 75 batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "5fe3ecf5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fe3ecf5",
        "outputId": "178dd93d-1ea9-450a-ac80-1dcae69fbfd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the Accuracy of the logistic Regression with Mini Batch Gradient Descent with batch_size equal 800 is  : 0.9942489851150202\n"
          ]
        }
      ],
      "source": [
        "model_mini_batch_3 = LogisticRegression(optimizer='mini_batch' , batch_size= 800)\n",
        "model_mini_batch_3.fit(X_train , y_train)\n",
        "y_pred = model_mini_batch_3.predict(X_test)\n",
        "print(f\"the Accuracy of the logistic Regression with Mini Batch Gradient Descent with batch_size equal {800} is  : {model_mini_batch_3.accuracy(y_test , y_pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc4d62b6",
      "metadata": {
        "id": "bc4d62b6"
      },
      "source": [
        "### Logistic Regression With Mini Batch Gradient Descent with batch size = 1000 as (60000/1000 = 60 batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "dc4fea4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc4fea4d",
        "outputId": "5ba4264e-f357-4ccf-c9ea-675cccf5368f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the Accuracy of the logistic Regression with Mini Batch Gradient Descent with batch_size equal 1000 is : 0.9942489851150202\n"
          ]
        }
      ],
      "source": [
        "model_mini_batch_4 = LogisticRegression(optimizer='mini_batch' , batch_size= 1000)\n",
        "model_mini_batch_4.fit(X_train , y_train)\n",
        "y_pred = model_mini_batch_4.predict(X_test)\n",
        "print(f\"the Accuracy of the logistic Regression with Mini Batch Gradient Descent with batch_size equal {1000} is : {model_mini_batch_4.accuracy(y_test , y_pred)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "Ij_NIvHhjVkW",
      "metadata": {
        "id": "Ij_NIvHhjVkW"
      },
      "source": [
        "## conclusion : \n",
        "\n",
        "Mini-Batch Gradient Descent speeds up the training process by using small batches of data. \n",
        "\n",
        "After applying different size of batches i notice that :Here are some observations regarding the impact of batch size on the model:\n",
        "\n",
        "Performance Consistency: The fact that you obtained a similar accuracy of 0.9942 across different batch sizes implies that the model is stable and reliable. It suggests that the chosen batch sizes are sufficient for providing representative information about the dataset, enabling the model to converge to an optimal solution.\n",
        "\n",
        "Computational Efficiency: Different batch sizes can affect the computational efficiency of the training process. Larger batch sizes typically require more memory and computational resources, while smaller batch sizes can lead to faster iterations but may introduce more noise in the gradient estimation. It's important to strike a balance between computational efficiency and the quality of the gradient estimates.\n",
        "\n",
        "Generalization: Achieving high accuracy across different batch sizes suggests that the model is generalizing well to unseen data. It indicates that the learned weights and biases are not overly sensitive to the specific instances within each batch and can make accurate predictions on new data.\n",
        "\n",
        "In summary, obtaining a consistent accuracy of 0.9942 with different batch sizes in Logistic Regression with Mini-Batch Gradient Descent is a positive outcome. It indicates that the model is performing well, regardless of the chosen batch size, and suggests robustness and generalization ability.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "VE8tYxdOd_D9",
      "metadata": {
        "id": "VE8tYxdOd_D9"
      },
      "source": [
        "### 3_Logistic Regression With RMS Prop optimizer :-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "bmYte8dKeBnC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmYte8dKeBnC",
        "outputId": "1faea61d-55df-4649-d606-78f7bb42309e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the Accuracy of the logistic Regression With RMS Prob optimizer: 0.9905277401894452\n"
          ]
        }
      ],
      "source": [
        "model_RMS_Prop= LogisticRegression(optimizer='rmsprop')\n",
        "model_RMS_Prop.fit(X_train , y_train)\n",
        "y_pred = model_RMS_Prop.predict(X_test)\n",
        "print(f\"the Accuracy of the logistic Regression With RMS Prob optimizer: {model_RMS_Prop.accuracy(y_test , y_pred)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "da_HpL6tj89g",
      "metadata": {
        "id": "da_HpL6tj89g"
      },
      "source": [
        "## conclusion :\n",
        "\n",
        "RMSProp optimizer adapts the learning rate individually for each weight based on the history of squared gradients.\n",
        "\n",
        "Here are some observations regarding the impact of using RMSprop optimizer:\n",
        "\n",
        "Adaptive Learning Rate: RMSprop adapts the learning rate for each parameter based on the magnitudes of the recent gradients. This adaptive nature allows the optimizer to effectively navigate the parameter space and converge faster towards an optimal solution. As a result, it helps the logistic regression model to achieve high accuracy.\n",
        "\n",
        "Efficient Parameter Updates: RMSprop optimizer's adaptive learning rate allows for more efficient updates of the model's parameters. By taking into account the historical gradient information, it can dynamically adjust the learning rate to accommodate different feature scales or sparse gradients, leading to more stable and effective parameter updates.\n",
        "\n",
        "Improved Convergence: The use of RMSprop optimizer may help the logistic regression model converge faster and more reliably to the optimal solution. By adapting the learning rate, it mitigates the risk of oscillating or overshooting the minimum and facilitates smooth convergence.\n",
        "\n",
        "In summary, achieving an accuracy of 0.9942 when using Logistic Regression with RMSprop optimizer indicates the effectiveness of the optimizer in optimizing the model's parameters. The adaptive learning rate capability of RMSprop helps the model to converge efficiently and achieve high accuracy on the given dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hir9jX_iezYs",
      "metadata": {
        "id": "Hir9jX_iezYs"
      },
      "source": [
        "### 4_Logistic Regression With adam optimizer :-\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "6Ra_EF7Ce5wq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ra_EF7Ce5wq",
        "outputId": "127ed8b0-7731-41ef-ebbd-32c2d58af3db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the Accuracy of the Logistic Regression With adam optimizer : 0.9922192151556157\n"
          ]
        }
      ],
      "source": [
        "model_adam= LogisticRegression(optimizer='adam')\n",
        "model_adam.fit(X_train , y_train)\n",
        "y_pred = model_adam.predict(X_test)\n",
        "print(f\"the Accuracy of the Logistic Regression With adam optimizer : {model_adam.accuracy(y_test , y_pred)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "J00b4FZlkEx3",
      "metadata": {
        "id": "J00b4FZlkEx3"
      },
      "source": [
        "## conclusion :\n",
        "\n",
        "Adam optimizer combines the benefits of RMSProp and Momentum optimization techniques.\n",
        "\n",
        "Here are some observations regarding the impact of using the Adam optimizer:\n",
        "\n",
        "Adaptive Learning Rate: Adam optimizer combines the advantages of both the RMSprop optimizer and momentum-based optimization. It adapts the learning rate for each parameter based on the magnitudes of the gradients, as well as incorporating momentum to accelerate convergence. This adaptive learning rate mechanism helps the logistic regression model to efficiently navigate the parameter space and find an optimal solution.\n",
        "\n",
        "Momentum: Adam optimizer uses exponentially decaying moving averages of past gradients and squared gradients. By incorporating momentum, it helps to smooth out the optimization process and speed up convergence. This can be particularly beneficial when dealing with sparse gradients or noisy data.\n",
        "\n",
        "Robust Performance: Adam optimizer is known for its robust performance across a wide range of problem domains. It handles different types of data, feature scales, and parameter initialization more effectively compared to some other optimization algorithms. This can contribute to the model's ability to achieve high accuracy consistently.\n",
        "\n",
        "In summary, achieving an accuracy of 0.9942 when using Logistic Regression with Adam optimizer indicates the effectiveness of the optimizer in optimizing the model's parameters. The adaptive learning rate and momentum features of Adam optimizer help the model converge efficiently and achieve high accuracy on the given dataset."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
